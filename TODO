- [ ] use argo workflow
- [x] implement docker for the test step
- [x] train model in the second step
- [x] add eveluation for robustness
- [x] add a branch with artificial error, and see the results in the flow steps
- [ ] document the argumentation for each step in a doc
- [ ] create flow for feature engineering
- [x] create a mechanism to introcude drift in data
- [ ] attach model versioning with CometML
- [ ] add logs to monitor when it makes sense and push to CometML
- [ ] i may want to have a feature store - or find an eaiser way to handle model training on different set of data
- [x] process the dataset, 
- [x] convert the dataset to a parquet type
- [x] use mlflow to log model and metrics
- [x] add a root requirements
- [ ] make sure the metadata is shared between flow steps, not the data
